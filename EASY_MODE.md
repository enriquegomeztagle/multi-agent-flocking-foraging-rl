# Easy Mode - Baseline Environment

## üìä Environment Overview

**Difficulty Level:** ‚≠ê Beginner
**Purpose:** Establish baseline performance with minimal competition and abundant resources
**Target Efficiency:** 70%+ (Expected: 80-90%)

---

## üéØ Objectives

1. **Validate Learning Capability**: Verify that PPO can learn effective foraging policies
2. **Establish Baseline**: Create performance benchmark for comparing harder difficulties
3. **Test Architecture**: Confirm multi-agent coordination without complex scenarios

---

## ‚öôÔ∏è Environment Configuration

### Core Parameters

```yaml
# Environment file: configs/env_easy.yaml
n_agents: 5              # Number of foraging agents
n_patches: 20            # Number of resource patches
width: 20.0              # World width
height: 20.0             # World height
episode_len: 2000        # Steps per episode

# Foraging parameters
feed_radius: 4.0         # Feeding range
c_max: 0.08              # Max consumption per step
S_max: 3.0               # Max patch capacity
regen_r: 0.4             # Regeneration rate

# Flocking parameters
k_neighbors: 4           # Neighbors for flocking
v_max: 1.5               # Max velocity
a_max: 0.2               # Max acceleration
turn_max: 0.3            # Max turn rate
d_safe: 0.6              # Safety distance
```

### Difficulty Characteristics

| Metric | Value | Analysis |
|--------|-------|----------|
| **Agent/Patch Ratio** | 0.25 (5/20) | Low competition - each agent has ~4 patches |
| **Patch Density** | 0.050 patches/unit¬≤ | High density - easy to find resources |
| **World Area** | 400 units¬≤ (20√ó20) | Small world - minimal travel time |
| **Feed Zone Area** | 50.3 units¬≤ (œÄr¬≤) | Large feeding radius - forgiving positioning |
| **Resource Availability** | 60 total capacity | Abundant - hard to deplete all patches |
| **Theoretical Max** | 800 (5√ó2000√ó0.08) | Maximum possible intake |

---

## üìà Evaluation Results

**Model:** [models/ppo_easy/final_model.zip](models/ppo_easy/final_model.zip)
**Evaluation:** 100 episodes
**Date:** 2025-01-12

### Performance Summary

| Metric | Value | vs Target |
|--------|-------|-----------|
| **Mean Efficiency** | **87.22%** | ‚úÖ +17.22% above target |
| **Median Efficiency** | **93.45%** | ‚úÖ Excellent consistency |
| **Mean Intake** | 697.75 ¬± 97.56 | 87% of theoretical max |
| **Median Intake** | 747.57 | 93% of theoretical max |
| **Min Intake** | 310.16 | 39% (worst case) |
| **Max Intake** | 800.01 | 100% (perfect episode!) |

### Performance Distribution

**Intake Statistics:**
- p25: 641.76 (80.22%)
- p50: 747.57 (93.45%)
- p75: 781.98 (97.75%)
- p90: 796.15 (99.52%)
- p95: 799.81 (99.98%)
- p99: 800.01 (100.00%)

**Performance Tiers:**
- üèÜ Excellent (‚â•70%): 80/100 episodes (80%)
- üåü Great (60-70%): 13/100 episodes (13%)
- ‚≠ê Good (50-60%): 4/100 episodes (4%)
- OK (40-50%): 2/100 episodes (2%)
- Below 40%: 1/100 episodes (1%)

### Top 5 Episodes

| Rank | Episode | Seed | Intake | Efficiency | Notes |
|------|---------|------|--------|------------|-------|
| ü•á | 38 | 1554 | 800.01 | 100.00% | Perfect score |
| ü•à | 73 | 3024 | 799.95 | 99.99% | Near perfect |
| ü•â | 76 | 3150 | 799.86 | 99.98% | Near perfect |
| 4 | 95 | 3948 | 799.82 | 99.98% | Near perfect |
| 5 | 59 | 2436 | 799.79 | 99.97% | Near perfect |

**Key Insight:** Multiple episodes achieve near-perfect or perfect efficiency!

---

## üî¨ Performance Analysis

### What Makes Easy Mode "Easy"?

1. **Low Competition** (5/20 = 0.25 ratio)
   - Each agent has ~4 patches available
   - Minimal resource conflicts
   - Agents rarely compete for same patch

2. **Small World** (20√ó20 = 400 area)
   - Average distance to nearest patch: 2-3 units
   - Minimal exploration time
   - Quick patch discovery

3. **Abundant Resources**
   - 20 patches √ó 3.0 capacity = 60 total units
   - Regeneration rate 0.4 = 40% per step
   - Resources regenerate faster than consumption

4. **Large Feed Radius** (4.0 units)
   - 50.3 unit¬≤ feeding area
   - Forgiving positioning
   - Agents don't need precise alignment

5. **Sufficient Time** (2000 steps)
   - Enough time to find all patches
   - No rush to maximize intake
   - Can afford inefficient paths

### Why 87% (Not 100%)?

The model achieves 87% mean efficiency (not 100%) due to:

- **Exploration Phase**: Initial steps spent discovering patch locations
- **Travel Time**: Time moving between patches (even if small)
- **Patch Depletion**: Temporary resource exhaustion requiring waiting/moving
- **Coordination Overhead**: Not all agents perfectly synchronized
- **Stochastic Variance**: Different random seeds create varied conditions

**Note:** 100% is theoretically impossible in practice - the 87% result is exceptional!

---

## üìä Comparison Baseline

This is the **baseline environment** - there's no "easier" version to compare to.

### vs Medium Mode (Preview)

| Metric | Easy | Medium | Change |
|--------|------|--------|--------|
| Agents | 5 | 10 | +100% |
| Patches | 20 | 18 | -10% |
| World Size | 20√ó20 (400) | 23√ó23 (529) | +32% area |
| Agent/Patch | 0.25 | 0.56 | +2.2x competition |
| Efficiency | 87.22% | 72.55% | -14.67pp |

**Progression:** Medium mode doubles agents and slightly reduces resources, creating moderate competition while maintaining good performance (72%).

---

## üéì Key Learnings

### Successes ‚úÖ

1. **Model Architecture Works**: Simple MLP policy achieves near-optimal performance
2. **Coordination Emerges**: Agents learn to distribute across patches without explicit communication
3. **Robust Learning**: 80% of episodes exceed 70% target
4. **Flocking Integration**: Reward structure successfully balances foraging and coordination

### Insights üí°

1. **Efficiency Ceiling**: 87% mean / 93% median suggests there's a natural upper limit even in easy scenarios
2. **Variance is Normal**: 97.56 std deviation indicates performance varies by seed/conditions
3. **Perfect Episodes Possible**: Several 100% episodes prove optimal policy is learnable
4. **Rapid Convergence**: Model learns effective policy relatively quickly

---

## üîß Training Details

**Model:** PPO (Proximal Policy Optimization)
**Policy:** MlpPolicy (Multi-Layer Perceptron)
**Training Time:** ~30-40 minutes
**Total Steps:** 1,500,000
**Parallel Envs:** 4

**Hyperparameters:**
- Learning Rate: 3e-4
- Batch Size: 256
- Gamma: 0.99
- N-Steps: 2048
- Clip Range: 0.2

---

## üìÅ Artifacts

- **Config:** [configs/env_easy.yaml](configs/env_easy.yaml)
- **Model:** [models/ppo_easy/final_model.zip](models/ppo_easy/final_model.zip)
- **Normalization:** [models/ppo_easy/vecnormalize.pkl](models/ppo_easy/vecnormalize.pkl)
- **Results:** [results/easy_evaluation.json](results/easy_evaluation.json)
- **Evaluation Script:** [train/eval_easy.py](train/eval_easy.py)

---

## üöÄ Usage

### Run Evaluation

```bash
python -m train.eval_easy --episodes 100
```

### Reproduce Best Episode

```python
from env.flockforage_parallel import FlockForageParallel, EnvConfig
import yaml

# Load config
with open('configs/env_easy.yaml', 'r') as f:
    config = yaml.safe_load(f)

env = FlockForageParallel(EnvConfig(**config))

# Perfect episode seed
obs, _ = env.reset(seed=1554)  # Episode 38: 100% efficiency
```

---

## ‚úÖ Conclusions

**Status:** ‚úÖ **SUCCESS**

Easy Mode achieves **87.22% mean efficiency**, significantly exceeding the 70% target. This demonstrates:

1. ‚úÖ RL approach is effective for multi-agent foraging
2. ‚úÖ Model architecture is appropriate
3. ‚úÖ Training methodology produces robust policies
4. ‚úÖ Baseline established for difficulty comparison

**Next Step:** Progress to Medium Mode (10 agents, 18 patches) to test scalability with moderate competition.
